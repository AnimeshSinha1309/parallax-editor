import os

from parallizer.utils import get_lm
from parallizer.utils.custom_lm_router import CustomLLMRouterLM
from parallizer.utils.lm_service import LM_API_BASE
from parallizer.signatures.completions_signature import InlineCompletion
from parallizer.fulfillers.base import Fulfiller
from shared.models import Card, CardType
from shared.context import GlobalPreferenceContext
from typing import List, Tuple, Optional
from abc import ABCMeta
import dspy
import logging

logger = logging.getLogger("parallax.completions")

COMPLETIONS_LM_MODEL = "openai/qwen-3-235b-a22b-instruct-2507"


# Create a combined metaclass to resolve the conflict between ABCMeta and dspy.Module's metaclass
class CombinedMeta(ABCMeta, type(dspy.Module)):
    """Combined metaclass for classes that inherit from both ABC and dspy.Module."""
    pass


class Completions(Fulfiller, dspy.Module, metaclass=CombinedMeta):

    def __init__(self, **kwargs):
        """Initialize the Completions fulfiller with DSPy module setup."""
        super().__init__(**kwargs)
        logger.info("Initializing Completions fulfiller")
        self._default_lm = get_lm()
        if self._default_lm is not None:
            logger.info("Default LM configured successfully")
        else:
            logger.warning("No default LM available during Completions initialization")

        self.completions_lm: Optional[CustomLLMRouterLM] = self._create_completions_lm(self._default_lm)
        if self.completions_lm is not None:
            logger.info("Completions fulfiller LM set to %s", COMPLETIONS_LM_MODEL)
        else:
            logger.warning("Failed to configure completions-specific LM '%s'", COMPLETIONS_LM_MODEL)
        self.predictor = dspy.Predict(InlineCompletion)

    async def forward(
        self,
        document_text: str,
        cursor_position: Tuple[int, int],
        global_context: GlobalPreferenceContext,
        intent_label: Optional[str] = None,
        **kwargs
    ) -> List[Card]:
        """
        Forward pass for completions fulfiller - generates inline completions.

        Args:
            document_text: The entire text content of the current document
            cursor_position: (line, column) position of the parser/cursor
            global_context: Global preference context containing scope root and plan path
            intent_label: Optional LLM-generated intent or label describing the query
            **kwargs: Additional parameters

        Returns:
            List of Card objects with completion results
        """
        logger.info(f"Completions fulfiller invoked at {cursor_position}, scope_root={global_context.scope_root}, plan_path={global_context.plan_path}")

        # Extract cursor context around parser position
        line, col = cursor_position
        lines = document_text.split('\n')

        logger.debug(f"Generating completion at cursor position {cursor_position}")

        # Build cursor context window (â‰¤150 characters)
        cursor_context = self._build_cursor_context(lines, line, col)
        logger.debug(f"Cursor context: {cursor_context[:100]}...")

        # Generate completion using DSPy Predict module
        logger.info("Invoking DSPy predictor for completion generation")
        completions_lm = self._get_completions_lm()
        if completions_lm is None:
            logger.error("Completions LM is unavailable; aborting completion generation")
            completions_lm = get_lm() # fallback to default LM

        with dspy.context(lm=completions_lm):
            result = self.predictor(
                full_document=document_text,
                cursor_context=cursor_context
            )

        # Convert to Card format
        cards = []
        if result.completion:
            logger.info(f"Completion generated: {result.completion[:50]}... (confidence: {result.confidence})")
            card = Card(
                header="Completion",
                text=result.completion,
                type=CardType.COMPLETION,
                metadata={
                    "confidence": result.confidence,
                    "intent_label": intent_label,
                    "cursor_position": cursor_position,
                }
            )
            cards.append(card)
        else:
            logger.warning("No completion generated by predictor")

        return cards
    
    def _build_cursor_context(self, lines: List[str], line: int, col: int) -> str:
        """Build cursor context window with <cursor> marker."""
        if line < 0 or line >= len(lines):
            return "<cursor>"
        
        current_line = lines[line]
        # Insert <cursor> marker at column position
        before_cursor = current_line[:col]
        after_cursor = current_line[col:]
        context_line = before_cursor + "<cursor>" + after_cursor
        
        # Try to include surrounding lines if within 150 char limit
        context = context_line
        if line > 0:
            prev_line = lines[line - 1]
            if len(prev_line + "\n" + context) <= 150:
                context = prev_line + "\n" + context
        
        if line < len(lines) - 1:
            next_line = lines[line + 1]
            if len(context + "\n" + next_line) <= 150:
                context = context + "\n" + next_line
        
        return context[:150]
    
    async def is_available(self) -> bool:
        """Check if completions fulfiller is available."""
        available = self._get_completions_lm() is not None
        logger.info(f"Completions fulfiller availability check: {available}")
        return available

    def _get_completions_lm(self) -> Optional[CustomLLMRouterLM]:
        """Return a completions-specific LM, attempting lazy reinitialization if needed."""
        if self.completions_lm is None:
            logger.info("Attempting to reinitialize completions-specific LM")
            if self._default_lm is None:
                self._default_lm = get_lm()
            self.completions_lm = self._create_completions_lm(self._default_lm)
        return self.completions_lm

    def _create_completions_lm(
        self,
        default_lm: Optional[object]
    ) -> Optional[CustomLLMRouterLM]:
        """Create the completions-specific LM instance targeting the Qwen model."""
        api_key = (
            os.getenv("CEREBRAS_API_KEY")
        )
        if not api_key:
            logger.warning("No API key found for configuring completions LM")
            return None

        api_base = (
            os.getenv("CEREBRAS_API_BASE")
        )

        try:
            return dspy.LM(
                api_base=api_base,
                api_key=api_key,
                model=COMPLETIONS_LM_MODEL,
                temperature=0.01,
            )
        except Exception as exc:
            logger.error(
                "Failed to instantiate completions LM '%s': %s",
                COMPLETIONS_LM_MODEL,
                exc,
                exc_info=True
            )
            return None
